# -*- coding: utf-8 -*-
"""HMM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EuaSiD008rO-KjJ5rS7G5g6JgoHb9uhS
"""
"""
Credit to Md Amimul Ehsan: Stock Market Prediction using HMM. https://www.kaggle.com/code/ehsanamim/stock-market-prediction-using-hmm
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from hmmlearn.hmm import GaussianHMM
from sklearn.preprocessing import StandardScaler

# Add open-close delta, open-high delta, and open-low delta to dataframe
def augment_features(dataframe):
    dataframe = dataframe.copy()
    fracocp = (dataframe['close']-dataframe['open'])/dataframe['open']
    frachp = (dataframe['high']-dataframe['open'])/dataframe['open']
    fraclp = (dataframe['open']-dataframe['low'])/dataframe['open']
    dataframe['delOpenClose'] = fracocp
    dataframe['delHighOpen'] = frachp
    dataframe['delLowOpen'] = fraclp
    return dataframe

# Only look at deltas and sentiment score
def extract_features(dataframe):
    return np.column_stack((dataframe['delOpenClose'], dataframe['delHighOpen'], dataframe['delLowOpen'], dataframe['sentiment_score']))

# Create train and test datasets
def create_hmm_dataset(tweet_data_src='data/sentiment/daily_sentiment_summary.csv'):
    # Define file paths (adjust these paths as necessary)
    stock_csv = 'data/stock/tsla.csv'

    # Load the sentiment CSV file, parsing the 'date' column as datetime objects
    sentiment_df = pd.read_csv(tweet_data_src, parse_dates=['day'])
    sentiment_df.rename(columns={'day': 'date'}, inplace=True)
    # Load the stock CSV file, also parsing the 'date' column
    stock_df = pd.read_csv(stock_csv, parse_dates=['date'])
    # Extract only the 'date' and 'change_close_to_close' columns from the stock data
    stock_subset = stock_df[['date', 'open', 'close', 'low', 'high']]
    stock_subset = augment_features(stock_subset)
    # # Merge the sentiment data with the selected stock data on the 'date' column.
    # # A left merge ensures all rows from sentiment_df are kept, and the corresponding
    # # change_close_to_close value is appended.
    # dataset = pd.merge(sentiment_df, stock_subset, on='date', how='left')

    # Merge using an inner join so that only rows with matching dates in both datasets remain.
    # NOTE: no stock data on weekends
    # TODO: consider averaging sentiment data over the weekend so that we don't completely ignore them.
    dataset = pd.merge(sentiment_df, stock_subset, on='date', how='inner')

    # TODO: Temporarily don't use the prob_negative,prob_neutral,prob_positive columns
    dataset.drop('prob_negative', axis=1, inplace=True)
    dataset.drop('prob_neutral', axis=1, inplace=True)
    dataset.drop('prob_positive', axis=1, inplace=True)

    # Sort by date to preserve time series order
    dataset = dataset.sort_values("date")

    # Split into 80% train, 20% test
    split_idx = int(len(dataset) * 0.8)
    train_df = dataset.iloc[:split_idx]
    test_df = dataset.iloc[split_idx:]

    # Save both splits
    train_df.to_csv("train_dataset.csv", index=False)
    test_df.to_csv("test_dataset.csv", index=False)

# Create dataframe
create_hmm_dataset()
df = pd.read_csv("train_dataset.csv")
df["date"] = pd.to_datetime(df["date"])
df = df.dropna(subset=["sentiment_score", 'delOpenClose', 'delHighOpen', 'delLowOpen'])
df = df.sort_values("date")

# Fit model to train dataset
model = GaussianHMM(n_components=10)

model.fit(extract_features(df))

import itertools

# Create sample space of possible outcomes from test dataset
test_data = pd.read_csv("test_dataset.csv")
test_data["date"] = pd.to_datetime(test_data["date"])
test_augmented = augment_features(test_data)
fracocp = test_augmented['delOpenClose']
frachp = test_augmented['delHighOpen']
fraclp = test_augmented['delLowOpen']

sample_space_fracocp = np.linspace(fracocp.min(), fracocp.max(), 50)
sample_space_fraclp = np.linspace(fraclp.min(), frachp.max(), 10)
sample_space_frachp = np.linspace(frachp.min(), frachp.max(), 10)
sentiment_range = np.linspace(test_augmented["sentiment_score"].min(),
                               test_augmented["sentiment_score"].max(), 10)

possible_outcomes = np.array(list(itertools.product(sample_space_fracocp, sample_space_frachp, sample_space_fraclp, sentiment_range)))

num_latent_days = 50
num_days_to_predict = 300

from tqdm import tqdm

# Determine most probable outcome (closing price) from sample space for 300 days, using previous 50 days as input
predicted_close_prices = []
for i in tqdm(range(num_days_to_predict)):
    # Calculate start and end indices
    previous_data_start_index = max(0, i - num_latent_days)
    previous_data_end_index = max(0, i)
    # Acquire test data features for these days
    previous_data = extract_features(augment_features(test_data.iloc[previous_data_start_index:previous_data_end_index]))

    outcome_scores = []
    for outcome in possible_outcomes:
        # Append each outcome one by one with replacement to see which sequence generates the highest score
        total_data = np.vstack((previous_data, outcome))
        outcome_scores.append(model.score(total_data))

    # Take the most probable outcome as the one with the highest score
    most_probable_outcome = possible_outcomes[np.argmax(outcome_scores)]
    predicted_close_prices.append(test_data.iloc[i]['open'] * (1 + most_probable_outcome[0]))

import matplotlib.pyplot as plt

plt.figure(figsize=(30,10), dpi=80)
plt.rcParams.update({'font.size': 18})

x_axis = pd.to_datetime(test_data.iloc[0:num_days_to_predict]['date'])
plt.plot(x_axis, test_data.iloc[0:num_days_to_predict]['close'], 'b+-', label="Actual close prices")
plt.plot(x_axis, predicted_close_prices, 'ro-', label="Predicted close prices")
plt.legend(prop={'size': 20})
plt.show()

from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(test_data['close'][:num_days_to_predict], predicted_close_prices)
print("MAE:", mae)

from sklearn.metrics import mean_squared_error
import numpy as np

mse = mean_squared_error(test_data['close'][:num_days_to_predict], predicted_close_prices)
rmse = np.sqrt(mse)
print("MSE:", mse)
print("RMSE:", rmse)

def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100

mape = mean_absolute_percentage_error(test_data['close'][:num_days_to_predict], predicted_close_prices)
print("MAPE:", mape)

